# 第四章 高级文件IO

## 分散聚集IO

分散聚集IO是一种可以在单次系统中对多个缓冲区输入输出的方法，可以把多个缓冲区的数据写到单个数据流，也可以把单个数据写进到多个缓冲区中，这种输入输出方法也称为向量IO，上一章的可以称为线性IO
>分散聚集IO优势在于：
编码模式更自然
效率更高
性能更好
支持原子性

* 使用readv()函数从文件描述符fd中读取count个段到参数iov所指定的缓冲区

```c
#include <sys/uio.h>

ssize_t readv(int fd, const struct iovec *iov, int count);
ssize_t writev(int fd, const struct iovec *iov, int count);

struct iovc{
    void *iov_base;     //指向段首的指针
    size_t iov_len;     //段长度
}
```

每个iovc结构体描述一个独立、物理不连续的缓冲区，我们称其为段segment。

readv函数在处理下个缓冲区之前，会填满当前缓冲区的iov_len个字节。writev则相反，填满当前缓冲区的iov_len个字节。
这两个函数都会顺序处理向量中的段，从iov下标0开始。

操作成功时，这两个函数都分别返回读写的字节数，等于count个iov_len的和，出错返回-1并设置errno。

实际上Linux内核中的所有IO都是向量IO，read()和write()都是作为向量IO实现的，且向量中只有一个段。

## event poll

epoll的实现要远复杂于poll和select，但是解决了基本的性能问题，并且增加了新的特性。

对于poll()和select(), 每次调用时都需要所有被听的文件描述符列表。内核必须遍历所有被监视的文件描述符。当这个文件描述符列表变得很大时包含几百个甚至几千个文件描述符时,每次调用都要遍历列表就变成规模上的瓶颈。
epoll把监听注册从实际监听中分离出来，从而解决了这个问题。一个系统调用会初始化epoll上下文，另一个从上下文中加人或删除监视的文件描述符，第三个执行真正的事件等待(event wait)

```c
#include <sys/epoll.h>
int epoll_create1(int flag);
int epoll_create(int szie);

```

调用成功后，epoll_create1会创建新的epoll实例，并返回和该实例关联的文件描述符，和真正的文件没有关系，只是为了后续调用epoll而创建。参数flags支持修改epoll的行为。
EPOLL_CLOEXEC表示进程被替换时关闭文件描述符。

出错时返回-1并设置errno。

epoll_create是epoll_create1老版本的实现，已经被废弃。不接受任何标志位。

* 使用epoll_ctl向指定的epoll上下文加入或者删除文件描述符

```c
#include <sys/epoll.h>
int epoll_ctl(int epfd,
              int op,
              int fd,
              struct epoll_event *event
              );
struct epoll_event{
  __u32 events;
  union{
    void *ptr,
    int fd,
    __u32 u32,
    __u64 u64
  } data;

}         

```

执行成功时，控制和文件描述符epfd相关联的epoll实例，op指定了对fd所指向的文件的执行的操作，如删除修改增加等，event规定了进一步的行为。

### 等待epoll事件

系统调用epoll_wait等待和指定epoll实例关联的文件描述符上的事件

```c
#include <sys/epoll.h>

int epoll_wait(int epfd,
               struct epoll_event *event,
               int maxevents,
               int timeout)
```

当调用epoll_wait()时，等待epoll实例epfd中的文件fd上的事件，时限为timeout毫秒，成功时events指向描述每个事件的epoll_event结构体的内存，最多有maxevents个事件，返回值是事件数，出错时返回errno并设置errno

>网络中复用IO大致流程如下：
1）用户空间调用epoll_create，内核新建epoll对象，返回epoll的fd，用于后续操作
2）用户空间反复调用epoll_ctl将我们要监听的fd维护到epoll，底层通过红黑树来高效的维护fd集合
3）用户空间调用epoll_wait获取就绪事件，内核检查epoll的就绪列表，如果就绪列表为空则会进入阻塞.
4）客户端向服务端发送数据，数据通过网络传输到服务端的网卡
5）网卡通过DMA的方式将数据包写入到指定内存中（ring_buffer），处理完成后通过中断信号告诉CPU有新的数据包到达
6）CPU收到中断信号后，进行响应中断，首先保存当前执行程序的上下文环境，然后调用中断处理程序（网卡驱动程序）进行处理：
a. 根据数据包的ip和port找到对应的socket，将数据放到socket的接收队列；
b. 执行socket对应的回调函数：将当前socket添加到eventpoll的就绪列表、唤醒eventpool等待队列里的用户进程（设置为RUNNING状态）
7）用户进程恢复运行后，检查eventpoll里的就绪列表不为空，则将就绪事件填充到入参中的events里，然后返回
8）用户进程收到返回的事件后，执行 events 里的事件处理，例如读事件则将数据从内核缓冲区拷贝到应用程序缓冲区
9）最后执行逻辑处理。

* 讨论IO模型，举个例子:

例子：你是一个老师，让学生做作业，学生做完作业后收作业。

>同步阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会等到他写完，然后才继续收下一个。
解析：这就是同步阻塞的特点，只要中间有一个未就绪，则你会被阻塞住，从而影响到后面的其他学生。
>同步非阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会跳过该学生，继续去收下一个。
解析：可以看到同步非阻塞相较于同步阻塞已经是更好的方案了，你不会因为某个学生未就绪而阻塞住，这样就可以减少对后续学生的影响。但是这个方案也可能会出现其他问题，如果你下去收作业的时候，全部学生都还没做完，则你可能会白走一圈，然后一个作业也没收到。
>select/poll：学生写完了作业会举手，但是你不知道是谁举手，需要一个个的去询问。
解析：这个方案相较于同步非阻塞来说有一点好处，就是你是确认有学生做完的，所以你下去肯定能收到作业，但是他有一个不好的点在于你需要一个个的去询问。
>epoll：学生写完了作业会举手，你知道是谁举手，你直接去收作业。
解析：这个方案就很高效了，每次都能准确的收到作业。

### 边缘触发事件和条件触发事件

如果epoll_ctl()的参数event中的events项设置为EPOLLET，fd上的监听方式为边缘触发 (Edge-triggered) ，否则为条件触发(Level-triggered)

也就是说，当有数据可读时，条件触发会直接返回，但是设置为边缘触发时，只有当数据写入时才会触发返回。

条件触发是默认行为，poll和select就是采用这种模式。

## 存储映射

除了标准文件IO，内核提供了一个接口，支持应用程序将文件映射到内存中，开发者可以直接通过内存来访问文件。

* 使用mmap()将对应映射到内存中：

```c
#include <sys/mman.h>

void *mmap(void *addr,
           size_t len,
           int prot,
           int flags,
           int fd,
           off_t offset);
```

参数prot规定了访存权限，flags指定了其他行为操作，addr告诉内核映射文件的最佳地址，但只是作为提示信息，大部分用户对该参数传递0。该调用返回内存映射区域的真实开始地址。

prot参数描述了对内存区域所请求的访问权限，如果是PROT_NONE表示无法访问映射区域的页。prot设置的权限和线程对于文件的访问权限不能冲突，如果以只读打开文件，就不能设定prot为PROT_WRITE。

flag规定了映射的类型，如映射区是否共享等。
MAP_SHARED和MAP_PRIVATE必须指定其中一个，但是不能同时指定。

