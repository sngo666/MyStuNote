# 第四章 高级文件IO

## 分散聚集IO

分散聚集IO是一种可以在单次系统中对多个缓冲区输入输出的方法，可以把多个缓冲区的数据写到单个数据流，也可以把单个数据写进到多个缓冲区中，这种输入输出方法也称为向量IO，上一章的可以称为线性IO
>分散聚集IO优势在于：
编码模式更自然
效率更高
性能更好
支持原子性

* 使用readv()函数从文件描述符fd中读取count个段到参数iov所指定的缓冲区

```c
#include <sys/uio.h>

ssize_t readv(int fd, const struct iovec *iov, int count);
ssize_t writev(int fd, const struct iovec *iov, int count);

struct iovc{
    void *iov_base;     //指向段首的指针
    size_t iov_len;     //段长度
}
```

每个iovc结构体描述一个独立、物理不连续的缓冲区，我们称其为段segment。

readv函数在处理下个缓冲区之前，会填满当前缓冲区的iov_len个字节。writev则相反，填满当前缓冲区的iov_len个字节。
这两个函数都会顺序处理向量中的段，从iov下标0开始。

操作成功时，这两个函数都分别返回读写的字节数，等于count个iov_len的和，出错返回-1并设置errno。

实际上Linux内核中的所有IO都是向量IO，read()和write()都是作为向量IO实现的，且向量中只有一个段。

## event poll

epoll的实现要远复杂于poll和select，但是解决了基本的性能问题，并且增加了新的特性。

对于poll()和select(), 每次调用时都需要所有被听的文件描述符列表。内核必须遍历所有被监视的文件描述符。当这个文件描述符列表变得很大时包含几百个甚至几千个文件描述符时,每次调用都要遍历列表就变成规模上的瓶颈。
epoll把监听注册从实际监听中分离出来，从而解决了这个问题。一个系统调用会初始化epoll上下文，另一个从上下文中加人或删除监视的文件描述符，第三个执行真正的事件等待(event wait)

```c
#include <sys/epoll.h>
int epoll_create1(int flag);
int epoll_create(int szie);

```

调用成功后，epoll_create1会创建新的epoll实例，并返回和该实例关联的文件描述符，和真正的文件没有关系，只是为了后续调用epoll而创建。参数flags支持修改epoll的行为。
EPOLL_CLOEXEC表示进程被替换时关闭文件描述符。

出错时返回-1并设置errno。

epoll_create是epoll_create1老版本的实现，已经被废弃。不接受任何标志位。

* 使用epoll_ctl向指定的epoll上下文加入或者删除文件描述符

```c
#include <sys/epoll.h>
int epoll_ctl(int epfd,
              int op,
              int fd,
              struct epoll_event *event
              );
struct epoll_event{
  __u32 events;
  union{
    void *ptr,
    int fd,
    __u32 u32,
    __u64 u64
  } data;

}         

```

执行成功时，控制和文件描述符epfd相关联的epoll实例，op指定了对fd所指向的文件的执行的操作，如删除修改增加等，event规定了进一步的行为。

### 等待epoll事件

系统调用epoll_wait等待和指定epoll实例关联的文件描述符上的事件

```c
#include <sys/epoll.h>

int epoll_wait(int epfd,
               struct epoll_event *event,
               int maxevents,
               int timeout)
```

当调用epoll_wait()时，等待epoll实例epfd中的文件fd上的事件，时限为timeout毫秒，成功时events指向描述每个事件的epoll_event结构体的内存，最多有maxevents个事件，返回值是事件数，出错时返回errno并设置errno

>网络中复用IO大致流程如下：
1）用户空间调用epoll_create，内核新建epoll对象，返回epoll的fd，用于后续操作
2）用户空间反复调用epoll_ctl将我们要监听的fd维护到epoll，底层通过红黑树来高效的维护fd集合
3）用户空间调用epoll_wait获取就绪事件，内核检查epoll的就绪列表，如果就绪列表为空则会进入阻塞.
4）客户端向服务端发送数据，数据通过网络传输到服务端的网卡
5）网卡通过DMA的方式将数据包写入到指定内存中（ring_buffer），处理完成后通过中断信号告诉CPU有新的数据包到达
6）CPU收到中断信号后，进行响应中断，首先保存当前执行程序的上下文环境，然后调用中断处理程序（网卡驱动程序）进行处理：
a. 根据数据包的ip和port找到对应的socket，将数据放到socket的接收队列；
b. 执行socket对应的回调函数：将当前socket添加到eventpoll的就绪列表、唤醒eventpool等待队列里的用户进程（设置为RUNNING状态）
7）用户进程恢复运行后，检查eventpoll里的就绪列表不为空，则将就绪事件填充到入参中的events里，然后返回
8）用户进程收到返回的事件后，执行 events 里的事件处理，例如读事件则将数据从内核缓冲区拷贝到应用程序缓冲区
9）最后执行逻辑处理。

* 讨论IO模型，举个例子:

例子：你是一个老师，让学生做作业，学生做完作业后收作业。

>同步阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会等到他写完，然后才继续收下一个。
解析：这就是同步阻塞的特点，只要中间有一个未就绪，则你会被阻塞住，从而影响到后面的其他学生。
>同步非阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会跳过该学生，继续去收下一个。
解析：可以看到同步非阻塞相较于同步阻塞已经是更好的方案了，你不会因为某个学生未就绪而阻塞住，这样就可以减少对后续学生的影响。但是这个方案也可能会出现其他问题，如果你下去收作业的时候，全部学生都还没做完，则你可能会白走一圈，然后一个作业也没收到。
>select/poll：学生写完了作业会举手，但是你不知道是谁举手，需要一个个的去询问。
解析：这个方案相较于同步非阻塞来说有一点好处，就是你是确认有学生做完的，所以你下去肯定能收到作业，但是他有一个不好的点在于你需要一个个的去询问。
>epoll：学生写完了作业会举手，你知道是谁举手，你直接去收作业。
解析：这个方案就很高效了，每次都能准确的收到作业。

### 边缘触发事件和条件触发事件

如果epoll_ctl()的参数event中的events项设置为EPOLLET，fd上的监听方式为边缘触发 (Edge-triggered) ，否则为条件触发(Level-triggered)

也就是说，当有数据可读时，条件触发会直接返回，但是设置为边缘触发时，只有当数据写入时才会触发返回。

条件触发是默认行为，poll和select就是采用这种模式。

## 存储映射

除了标准文件IO，内核提供了一个接口，支持应用程序将文件映射到内存中，开发者可以直接通过内存来访问文件。

* 使用mmap()将对应映射到内存中：

```c
#include <sys/mman.h>

void *mmap(void *addr,
           size_t len,
           int prot,
           int flags,
           int fd,
           off_t offset);
```

参数prot规定了访存权限，flags指定了其他行为操作，addr告诉内核映射文件的最佳地址，但只是作为提示信息，大部分用户对该参数传递0。该调用返回内存映射区域的真实开始地址。

prot参数描述了对内存区域所请求的访问权限，如果是PROT_NONE表示无法访问映射区域的页。prot设置的权限和线程对于文件的访问权限不能冲突，如果以只读打开文件，就不能设定prot为PROT_WRITE。

flag规定了映射的类型，如映射区是否共享等。
MAP_SHARED和MAP_PRIVATE必须指定其中一个，但是不能同时指定。

## 页大小

页是内存管理单元(MMU)的粒度，也是内存中允许具有不同权限和行为的最小单位。

mmap系统调用的操作单元是页，参数addr和offset都必须按页大小对齐，也就是说必须是页大小的整数倍。如果len不是，会一直占满最后一整个页。

使用sysconf()获得页的大小:

```c
#include <unistd.h>

long sysconf(int name);
long page_size = sysconf (_SC_PAGESIZE);

```

POSIX 定义SC PAGESIZESC PAGE SIZE 与其同义)表示页大小。因此，在运行时获取页大小其实很简单:
Linux也提供了getpagesize()函数来获得页大小:

```c
#include <unistd.h>

int getpagesize (void);
int page_size = getpagesize ();
```

调用getpagesize将返回页按字节计数的大小。使用也比sysconf简单。
并不是所有的UNIX系统都支持这个函数，POSIX 1003.1-2001弃用了该函数。

页大小是由<asm/pages.h>中的宏 PAGE_SIZE 定义的。因此，第三种获取页大小的方式是:

```c
#include <asm/pages.h>

int page_size=PAGESIZE;

```

和前两种方式不同，这种方法是在编译时获得页大小，而不是在运行时。一些体系结构支持多种机型使用不同页大小，某些机型本身甚至支持多种页大小。
一个二进制文件应该能在指定体系结构下的所有机型上运行，即一次编译，到处运行。对页大小硬编码则会使这种可能性为0。因此，正确的做法是在运行时确定页大小。因为参数addr和offset通常设置为0，在运行时确定其实并不是很困难。

* 使用munmap系统调用来取消mmap所创建的映射

```c
#include <sys/mman.h>

void *munmap(void *addr, size_t len);
```

munmap会消除进程地址空间从addr开始len字节长的内存中所有页面的映射。一旦映射被消除之前关联的内存区域不再有效，如果试图再次访问就会生成SIGSEGV信号。

一般来说传递给munmap参数就是上一次用于mmap的返回值以及len
成功时返回0，失败返回-1设置errno。

### mmap的优缺点

相对于系统调用read和write而言，有许多优点

1. 使用read()或 write()系统调用时，需要从用户缓冲区进行数据读写，而使用映射文件进行操作，可以避免多余的数据拷贝操作。
2. 除了可能潜在页错误，读写映射文件不会带来系统调用和上下文切换的开销它就像直接操作内存一样简单。
3. 当多个进程把同一个对象映射到内存中时，数据会在所有进程间共享。只读和写共享的映射在全体中都是共享的;私有可写的映射对尚未进行写时拷贝的页是共享的。
4. 在映射对象中搜索只需要很简单的指针操作，不需要使用系统调用lseek()

缺点:

1. 由于映射区域的大小总是页大小的整数倍，因此，文件大小与页大小的整数倍之间有空间浪费。对干小文件，空间浪费会比较严重。例如，如果页大小是4KB，一个7字节的映射就会浪费4096字节
2. 存储映射区域必须在进程地址空间内。对于32位的地址空间，大量的大小不同的映射会导致生成大量的碎片，使得很难找到连续的大片空内存。当然，这个问题在64位地址空间就不是很明显。
3. 创建和维护映射以及相关的内核数据结构有一定的开销。不过，如上节所述由于mmap()消除了读写时的不必要拷贝，这种开销几乎可以忽略，对于大文件和频繁访问的文件更是如此。

### 调整映射的大小

使用mremap来扩大或者减小指定映射的大小，用以将[addr, addr + old_size)的大小增加或者减少到new_size，这是从低地址开始到高地址结束的。
参数flags的值可以是0或者MREMAP_MAYMOVE,后者表示内核可以根据需求移动映射区域，设置为指定大小。

```c
#define _GNU_SOURCE

#include <sys/mman.h>

void * mremap(void *addr, size_t old_size, size_t new_size, unsigned long flags);

```

glibc使用mremap()实现高效的realloc。

### 改变映射区域权限

POSIX规定了mprotect()接口，允许程序改变已有内存区域的权限。addr是页对齐的，参数prot和mmap接收的prot参数相同，这些值都不能累加。

```c
#include <sys/mman.h>

int mprotect(const void *addr,
             size_t len,
             int prot)
```

### 通过映射同步文件

```c
#include <sys/mman.h>

int msync(const void *addr,
             size_t len,
             int flags)
```

调用msync可以将mmap生成的映射在内存中的任何修改写回磁盘中，实现同步内存中的映射和被映射的文件。参数addr必须是页对齐的。

如果不调用msync()，无法保证在映射被取消之前，修改过的映射会被写回到硬盘这一点与write()有所不同，被write()修改的缓冲区被保存在一个队列中等待被写回。而当向内存映射写数据时，进程会直接修改内核页缓存中的文件页，而无需经过内核。内核不会立即同步页缓存到硬盘。

参数flag控制同步操作的行为。例如MS_SYNC要求必须同步完成后返回。
失败返回-1并设置errno。

### 给出映射提示

Linux提供了系统调用madvise(),进程对自己期望如何访问映射区域给内核一些提示信息。内核会据此优化自己的行为，尽量更好地利用映射区域。内核通常会动态调整自己的行为，一般而言，即便没有显式提示信息，内核也能保证较好的性能但是适当的提示信息可以确保在某些负载情况下，可以获得期望的缓存并准确预读。

调用madvise()会告诉内核该如何对起始地址为addr，长度为len的内存映射区域进行操作。

```c
#include <sys/mman.h>

int madvise(const void *addr,
             size_t len,
             int advice)
```

参数advice表示提示信息：
MADV_NORMAL内核行为照常，有适量的预读。
MADV_RANDOM 内核不做预读，每次物理读操作只读取最小量的数据。
MADV_SEQUENTIAL 内核大量预读。
MADV_WILLNEED 内核开始预读，将指定的页预读至内存
MADV_DONTNEED 内核释放所有和指定页相关的资源，丢弃所有被修改的未同步写回页。后续对映射数据的访问会把数据重新载入内存页或以0填充请求页。

失败返回-1并设置errno。

## 普通文件IO提示

通过使用posix_fadvise()和readahead()在使用普通文件IO时，如何给内核提供操作提示。

* 系统调用posix_fadvise()

```c
#include <fcntl.h>

int posix_fadvise(int fd,
                  off_t offset,
                  off_t len,
                  int advice);
```

该调用会向内核提供在文件fd的[offset, offset + len)区间内的操作提示，如果len为0，则提示适用于区间[offset,length of file]，常见的是设置len和offset为0，使提示适用于整个文件。

* readahead()

该调用用以将fd所表示文件的映射区域读入到页缓存中,相当于posix_fadvise使用POSIX_FADV_WILLNEED

```c
#define _GNU_SOURCE

#include <fcntl.h>

ssize_t readahead(int fd,
                  off64_t offset,
                  size_t count);
```

### 经济实用的操作提示

通过向内核传递良好的操作提示，很多普通应用的效率可以获得明显提升。这种提示信息对于减轻繁重的I/O负荷很有助益。由于磁盘速度(很慢)与现代处理器速度 (很快)的不匹配，每个提示位的设置都很重要，良好的提示信息对应用大有帮助。
在读取文件的一个块的内容时，进程可以通过设置 POSIX_FADV_WILLNEED告诉内核把文件预读到页缓存中。预读的 I/O操作将在后台异步进行。当应用最终要访问文件时，访问操作可以立即返回，不会有IO阻塞。
相反地，在读写大量数据后 (比如往磁盘写入连续的视频数据流)，进程可以设置POSIX_FADV_DONTNEED，告诉内核丢弃页面缓存中指定文件块的内容。大量的流操作会连续填满页缓冲区。如果应用不想再次访问这些数据，则意味着页缓冲区中充斥了过量的数据，其代价是导致没有空间保存有用的数据。因此对于视频流这类应用，应该定期请求将数据从缓存中清除。
如果一个进程想要读取整个文件时，可以设置POSIX_FADV_SEQUENTIAL，告诉内核要大量预读。相反地，如果一个进程知道自己将随机访问文件，可以设置POSIX_FADV_RANDOM，告诉内核预读没有用，只会带来无谓的开销。

## 同步和异步操作

读写操作通常是`synchronous`或`nonsynchronous`与`synchronized`或`nonsynchronized`的排列组合  

同步`synchronous`写操作在数据全部写到内核缓冲区之前是不会返回的，同步读操作在数据写到应用程序到用户空间的缓冲区之前是不会返回的。异步操作则相反，在异步操作时请求并没有放入操作队列中来执行，当然这种情况下会有机制监督异步操作的最终完成。

`synchronized`比synchronous操作更加严格，也更加安全。synchronized写操作把数据写回硬盘，确保硬盘上数据和内核缓冲区是一致的，synchronized读操作总是从硬盘上读取数据。

通常UNIX的写操作是synchronous但nonsynchronized,读操作是synchronous且synchronized。

执行异步IO需要内核在最底层的支持，aio库提供了一系列函数来实现异步IO。
`#include <aio.h>`

## IO调度器和IO性能

在现代系统中，磁盘和其他组件的性能差距很大，而且还在逐渐增大，其最糟糕的部分在于把读写头从磁盘的一个地方移动到另一个地方，该操作被称为seek，即定位，单次磁盘查找定位在8毫秒以上，而一次处理器周期大概是三分之一纳秒，前者会是后者的2500万倍。

### 磁盘寻址

硬盘基于用柱面cylinders、磁头heads和扇区section几何寻址方式CHS来获取数据，每个磁盘有多个盘片组成，每个盘片分别只包含一个磁盘，一个主轴和一个读写头，盘片相当于一个cd。每个盘片分成许多环状的磁道，每个磁道分成整数倍个扇区。

为了定位特定数据单元在磁盘上的位置，驱动程序需要知道三个信息，柱面、磁头和扇区的值，柱面指定了数据在哪条磁道上，但是并不知道在哪个磁盘上，相当于一个圆柱体中到中轴线固定距离组成的柱面。磁头表示准确的读写头，也就是准确的盘片，于是这就找到了具体盘片上具体的磁道。然后定位读写头找到正确的扇区。

现代系统不会直接操作磁盘的柱面、磁头和扇区。硬盘驱动将每个柱面/磁头/扇区的三元组映射成唯一的号，叫做物理块或者设备号，现代操作系统可以直接使用块号即逻辑块寻址LBA来访问硬盘，硬盘驱动程序把块号转换成正确的CHS地址。而块数量的容纳量就是今年来磁盘容量上限的限制。块到CHS的映射是连续的，相邻的逻辑块在物理上也是连续的。

逻辑块的数量必须是物理块数量的整数倍。

### IO调度器

IO调度器主要实现两个功能，合并merging和排序sorting。

合并merging是将两个或多个响铃的IO请求过程合并为一个，比如对于5号和6到7逻辑块的请求，合并为5到7的请求，IO吞吐量没有变，但是显著减少了IO请求的数量。

排序sorting是选取两个操作中相对重要的一个，并以块号递增的顺序重新安排等待的IO请求。

以这种方式磁头移动的剧情最小，以平滑线性的方式移动。

### 改进读请求

当读请求请求的数据不在页缓存中时，从磁盘中读出数据前会一直处于堵塞状态。一个应用可能会在短时间内发出多个读操作请求，请求的完成时串行的，且在每个操作完成返回数据之前不会处理下一个请求，这会导致相当大的读延迟。
因此IO调度器采用了一种机制以避免饿死现象。

* Deadline I/O调度器

Linus电梯算法维护了一个经过排序的IO等待列表，队首的请求是将要被调度的。Deadline I/O保留了这个队列，并增加了两个新的队列读FIFO和写FIFO，也就是说队列中的请求是按时间顺序排列的，并为每个请求设置了一个过期时间，一个请求会被分别加入标准队列和对应的读写FIFO队列中。

当某个FIFO队列的队首请求超出了所在队列的过期时间时，IO调度器会停止从标准IO队列中调度请求，转而调度这个FIFO队列的队首请求。而这个FIFO队列的队首请求，就是队列中等待最久的。

这种调度方式能较为有效保证饿死的发生。

* Anticipatory I/O调度器

Deadline的问题在于，当前一个读请求返回后才会执行下一个读请求，当前一个完成后，调度器可能又去处理别的请求， 这导致每次读取都要进行不必要的寻址操作，查找数据，读取，返回。
因此Anticipatory在原先基础上增加了预测机制，当提交一个读请求时，当请求返回后调度器会等待6毫秒，如果应用程序在等待期间对硬盘同一部分发起另外一次读请求，读请求就会被立即相应。如果没有相关请求，则正常返回。

* CFQ I/O调度器

Compete Fair Queuing完全公平序列和上述两种调度器的目标是一致的。
使用CFQ时，每个进程都有自己的队列，每个队列都分配一个时间片。IO调度器使用轮询方式访问并处理队列中的请求，直到队列的时间片耗尽或者所有的请求都被处理完，这种情况下会空转10毫秒，然后继续等待处理新的请求。

在每个进程的队列中，同步请求被赋予比非同步请求更高的优先级。调度器对于所有的进程都是公平的，全局性能也相当优秀。

* Noop I/O调度器

该调度程序是现在最简单的调度程序，无论什么时候都不进行排序操作，只做简单的合并，一般用在需求简单的设备上。

### 选择和配置调度器

在启动时可以通过内核命令参数iosched来指定默认的IO调度器，有效选项包括as、cfq、deadline和noop，运行时针对块设备选择，通过修改`/sys/block/device/queue/scheduler`来完成。

```bash
echo cfq > /sys/block/device/queue/scheduler
```

### 优化IO性能

通过减少IO操作的次数，实现块对齐的IO或者使用用户空间进行缓冲，甚至利用高级IO技术来使IO性能达到最优是十分重要的。

应用在持续向IO调度器发起请求，然而IO调度器在完成合并和排序后向磁盘转发请求后，应用仍在不停提交请求，因此每次调度器排序的请求可能只是请求中的一小部分，其余的都要被挂起。
因此如果某个应用需要在短时间内提交大量请求，尤其是可能遍布整个磁盘的数据，最好再提交之前就对其进行排序。但是，对于同样的信息，用户空间的程序和内核不见得有同样的访问权限。在I/0调度器的最底层，请求已经是以物理块的形式进行组织。对物理块进行排序是很简单的。但是，在用户空间请求是以文件和文件偏移的形式存在的。用户空间的应用必须获取信息，并对文件系统的布局做出合理的猜测。

用户空间应用对于IO请求可以作出很多种排序处理以优化

* 按路径排序

这是最简单的，也是最低效的接近快排序的方式，在大部分文件系统所采用的布局算法中，每个目录里的文件往往在磁盘上相邻，同一个目录中的文件，如果在同一时间内被创建，物理相邻的概率往往更高。

这种方式的缺点在于没有考虑文件系统的碎片，碎片越多按路径排序的作用就越小，优点在于按路径排序对于所有的文件系统都是可用的，不管文件布局上是否物理相邻，空间局部性使得这种方式至少比较准确。

* 按inode排序

inode是包含和文件唯一相关的元信息的结构，一个文件可能占用多个物理块，但是只会拥有一个inode。
可以通过stat()系统调用来获得inode序号。

```c
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/types.h>
#include <sys/stat.h>

 
int get_inode (int fd){
    struct stat buf;
    int ret;

    ret = fstat(fd,&buf);
    if(ret < 0){
        perror("fstat");
        return -1;
    }
}
```

按inode编号排序有如下优点:inode编号容易获取，容易排序，和文件的物理布局很近似。
主要的缺点是碎片会降低这种近似性,而且近似性只是估算,在非UNIX系统上也不够准确。无论如何，使用inode进行排序都是在用户空间IO请求调度中最常用的方法。

* 按物理块排序

设计自己的电梯算法，最好的方式是使用物理块进行排序。逻辑块的大小和文件系统有关，每个逻辑块对应一个物理块。
内核提供了通过文件的逻辑块获得物理块的方法，通过系统调用ioctl()

```c
ret = ioctl(fd, FIBMAP, &block);
if (ret < 0)
  perror("ioctl");

```

block是确定其物理块号的逻辑块，调用成功返回的物理块号存储其内。逻辑块索引从0开始。
使用FIBMAP的缺点在于它需要设置CAPSYSRAWIO权限——即拥有root权限。因此，非root的应用无法使用这种方法。此外，虽然FIBMAP命令是标准化的，但是其具体的实现则是和每个文件系统相关。虽然常见的文件系统如ext2和ext3都支持FIBMAP，但无法避免某些离奇的文件系统不支持的情况。如果不支持FIBMAP，ioctl()会返回EINVAL。
